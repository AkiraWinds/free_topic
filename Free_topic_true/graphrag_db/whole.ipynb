{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63b814a",
   "metadata": {},
   "source": [
    "# 1. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde02037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaz/Desktop/free_topic/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset loaded successfully!\n",
      "Available splits: ['corpus']\n",
      "Using split: 'corpus', Total rows: 232560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 233/233 [01:29<00:00,  2.60ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "print(\"start loading dataset...\")\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"isaacus/open-australian-legal-corpus\")\n",
    "\n",
    "print(f\"✓ Dataset loaded successfully!\")\n",
    "print(f\"Available splits: {list(ds.keys())}\")\n",
    "\n",
    "# Get the first split (corpus, train, etc.)\n",
    "split_name = list(ds.keys())[0]\n",
    "dataset = ds[split_name]\n",
    "print(f\"Using split: '{split_name}', Total rows: {len(dataset)}\")\n",
    "\n",
    "#store dataset into data/raw as json\n",
    "os.makedirs(\"../data/raw\", exist_ok=True)\n",
    "output_path = \"../data/raw/1raw_corpus.jsonl\"\n",
    "\n",
    "dataset.to_json(output_path, force_ascii=False)\n",
    "\n",
    "print(f\"✓ Data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f265b",
   "metadata": {},
   "source": [
    "only selece data: type = primary_legislation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f1c3e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4v/p8sw58qs2l50qg9jlb8wbyjm0000gp/T/ipykernel_60090/445732772.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  filtered_dataset = pd.concat(chunks, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows: 1418\n",
      "✓ Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Reading in chunks...\")\n",
    "chunks = []\n",
    "for chunk in pd.read_json(\"../data/raw/1raw_corpus.jsonl\", lines=True, chunksize=10000):\n",
    "    filtered_chunk = chunk[(chunk['type'] == 'primary_legislation') & (chunk['jurisdiction'] == 'new_south_wales')]\n",
    "    chunks.append(filtered_chunk)\n",
    "\n",
    "# Combine and sort\n",
    "filtered_dataset = pd.concat(chunks, ignore_index=True)\n",
    "filtered_dataset = filtered_dataset.sort_values('date', ascending=False)\n",
    "\n",
    "print(f\"Filtered rows: {len(filtered_dataset)}\")\n",
    "\n",
    "# Save\n",
    "filtered_dataset.to_json(\"../data/raw/2primary_legislation_new_south_wales.jsonl\", \n",
    "                         orient='records', lines=True, force_ascii=False)\n",
    "print(\"✓ Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec369d5",
   "metadata": {},
   "source": [
    "select 100 piece of data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e126bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in chunks...\n",
      "✓ Random sample saved!\n"
     ]
    }
   ],
   "source": [
    "# select 100 piece of data randomly\n",
    "print(\"Reading in chunks...\")\n",
    "chunks = []\n",
    "for chunk in pd.read_json(\"../data/raw/2primary_legislation_new_south_wales.jsonl\", lines=True, chunksize=140):\n",
    "    chunks.append(chunk.sample(n=10, random_state=42))  # select 10 samples from each chunk\n",
    "\n",
    "# Combine all chunks into a single DataFrame\n",
    "random_sample = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "random_sample.to_json(\"../data/raw/4primary_legislation_new_south_wales_random_sample2.json\", \n",
    "                         orient='records', lines=False, force_ascii=False)\n",
    "print(\"✓ Random sample saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0fbff2",
   "metadata": {},
   "source": [
    "only citation and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "560d71c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in chunks...\n",
      "Filtered rows: 1418\n",
      "✓ Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Reading in chunks...\")\n",
    "chunks = []\n",
    "for chunk in pd.read_json(\"../data/raw/1raw_corpus.jsonl\", lines=True, chunksize=10000):\n",
    "# only citation and url column\n",
    "    filtered_chunk = chunk[(chunk['type'] == 'primary_legislation') & (chunk['jurisdiction'] == 'new_south_wales')]\n",
    "    filtered_chunk = filtered_chunk[['citation', 'url']]\n",
    "    chunks.append(filtered_chunk)\n",
    "\n",
    "# Combine\n",
    "filtered_dataset = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "print(f\"Filtered rows: {len(filtered_dataset)}\")\n",
    "\n",
    "# Save\n",
    "filtered_dataset.to_json(\"../data/raw/2primary_legislation_new_south_wales_filtered.json\", \n",
    "                         orient='records', lines=False, force_ascii=False)\n",
    "print(\"✓ Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6510613d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in chunks...\n",
      "Filtered rows: 3\n",
      "✓ Done!\n",
      "Filtered rows: 3\n",
      "✓ Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4v/p8sw58qs2l50qg9jlb8wbyjm0000gp/T/ipykernel_60090/3816145651.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  filtered_dataset = pd.concat(chunks, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Reading in chunks...\")\n",
    "chunks = []\n",
    "for chunk in pd.read_json(\"../data/raw/1raw_corpus.jsonl\", lines=True, chunksize=10000):\n",
    "    # 修复：用 | 代替 or，并给每个条件加括号\n",
    "    filtered_chunk = chunk[\n",
    "        (chunk['citation'] == 'Children (Protection and Parental Responsibility) Act 1997 (NSW)') |\n",
    "        (chunk['citation'] == 'Children (Education and Care Services National Law Application) Act 2010 (NSW)') |\n",
    "        (chunk['citation'] == 'Children (Education and Care Services) Supplementary Provisions Act 2011 (NSW)')\n",
    "    ]\n",
    "    chunks.append(filtered_chunk)\n",
    "\n",
    "# Combine\n",
    "filtered_dataset = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "print(f\"Filtered rows: {len(filtered_dataset)}\")\n",
    "\n",
    "# Save\n",
    "filtered_dataset.to_json(\"../data/raw/primary_legislation_new_south_wales.json\", \n",
    "                         orient='records', lines=False, force_ascii=False, indent=2)\n",
    "print(\"✓ Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dcc4e9",
   "metadata": {},
   "source": [
    "data preprocessing: data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee6b3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "def normalize_legislation_text(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Rules:\n",
    "    1) If a line begins with \"Part <num>\" -> prefix with \"[PART] \"\n",
    "    2) If a line begins with \"Division <num>\" -> prefix with \"[DIVISION] \"\n",
    "    3) If a line begins with \"<num> \" or \"<num><letter> \" (e.g., 9A, 29CA) -> prefix with \"[SECTION] \"\n",
    "    4) If newline is followed by 4 spaces (i.e. '\\n    ') -> treat as indented text; add ':' before that indented block\n",
    "    5) Remove all '\\n' and extra spaces in the final output\n",
    "    \"\"\"\n",
    "    if not raw_text:\n",
    "        return \"\"\n",
    "\n",
    "    # Normalize line endings\n",
    "    text = raw_text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # Convert indented newlines into a colon separator before the indented text.\n",
    "    # Example: \"Title\\n    body...\" -> \"Title: body...\"\n",
    "    #text = re.sub(r\"\\n[ ]{4,}\", \": \", text)\n",
    "\n",
    "    # Now we can safely split by remaining newlines (these were your '\\n\\n' / new paragraph markers).\n",
    "    lines = [ln.strip() for ln in text.split(\"\\n\") if ln.strip()]\n",
    "\n",
    "    out_lines: List[str] = []\n",
    "\n",
    "    part_pat = re.compile(r\"^Part\\s+\\d+\\b\", re.IGNORECASE)\n",
    "    div_pat = re.compile(r\"^Division\\s+\\d+\\b\", re.IGNORECASE)\n",
    "    # Sections like \"1 Short title\", \"9A Application ...\", \"29CA APRA may request ...\"\n",
    "    sec_pat = re.compile(r\"^(?:\\d+[A-Z]{0,3})\\b\")\n",
    "\n",
    "    for ln in lines:\n",
    "        if part_pat.match(ln):\n",
    "            out_lines.append(f\"[PART] {ln}\")\n",
    "        elif div_pat.match(ln):\n",
    "            out_lines.append(f\"[DIVISION] {ln}\")\n",
    "        elif sec_pat.match(ln):\n",
    "            # Only tag if it *starts* with a section-like token (e.g., \"10 Definitions\")\n",
    "            out_lines.append(f\"[SECTION] {ln}\")\n",
    "        else:\n",
    "            out_lines.append(ln)\n",
    "\n",
    "    # Join and collapse any remaining whitespace\n",
    "    normalized = \" \".join(out_lines)\n",
    "    normalized = re.sub(r\"\\s+\", \" \", normalized).strip()\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def process_json_file(\n",
    "    input_path: Union[str, Path],\n",
    "    output_path: Union[str, Path],\n",
    "    text_key: str = \"text\",\n",
    "    output_key: str = \"text_normalized\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads a JSON file that is either:\n",
    "      - a list[dict] records, or\n",
    "      - a dict with a list under some key (common pattern: {\"data\": [...]})\n",
    "    For each record, reads record[text_key], writes record[output_key].\n",
    "    \"\"\"\n",
    "    input_path = Path(input_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    with input_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data: Any = json.load(f)\n",
    "\n",
    "    def handle_records(records: List[Dict[str, Any]]) -> None:\n",
    "        for rec in records:\n",
    "            raw = rec.get(text_key, \"\")\n",
    "            rec[output_key] = normalize_legislation_text(raw)\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        handle_records(data)\n",
    "    elif isinstance(data, dict):\n",
    "        # If it's a dict, try common patterns: \"data\", \"records\", otherwise treat as single record if it has text_key\n",
    "        if isinstance(data.get(\"data\"), list):\n",
    "            handle_records(data[\"data\"])\n",
    "        elif isinstance(data.get(\"records\"), list):\n",
    "            handle_records(data[\"records\"])\n",
    "        elif text_key in data:\n",
    "            data[output_key] = normalize_legislation_text(data.get(text_key, \"\"))\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unrecognized JSON structure. Expected list or dict with key '{text_key}' \"\n",
    "                f\"or list under 'data'/'records'. Got keys: {list(data.keys())[:30]}\"\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported JSON root type: {type(data)}\")\n",
    "\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# process_json_file(\n",
    "#     input_path=\"/Users/yaz/Desktop/free_topic/Free_topic_true/data/raw/4primary_legislation_random_sample.json\",\n",
    "#     output_path=\"/Users/yaz/Desktop/free_topic/Free_topic_true/data/raw/4primary_legislation_random_sample.normalized.json\",\n",
    "#     text_key=\"text\",\n",
    "#     output_key=\"text_normalized\",\n",
    "# )\n",
    "process_json_file(\n",
    "    input_path=\"../data/raw/act-1997-078/primary_legislation_new_south_wales.json\",\n",
    "    output_path=\"../data/processed/act-1997-078/act-1997-078_normalized.json\",\n",
    "    text_key=\"text\",\n",
    "    output_key=\"text_normalized\",\n",
    ")\n",
    "process_json_file(\n",
    "    input_path=\"../data/raw/act-2010-104/primary_legislation_new_south_wales.json\",\n",
    "    output_path=\"../data/processed/act-2010-104/act-2010-104_normalized.json\",\n",
    "    text_key=\"text\",\n",
    "    output_key=\"text_normalized\",\n",
    ")\n",
    "process_json_file(\n",
    "    input_path=\"../data/raw/act-2011-070/primary_legislation_new_south_wales.json\",\n",
    "    output_path=\"../data/processed/act-2011-070/act-2011-070_normalized.json\",\n",
    "    text_key=\"text\",\n",
    "    output_key=\"text_normalized\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449b795a",
   "metadata": {},
   "source": [
    "read csv and transform into excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cfb829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv and transform into excel\n",
    "import pandas as pd\n",
    "node = pd.read_csv('../data/neo4j_data/Document_Structure_Graph3/nodes.csv')\n",
    "node.to_excel('../data/neo4j_data/Document_Structure_Graph3/nodes.xlsx', index=False)\n",
    "\n",
    "rel = pd.read_csv('../data/neo4j_data/Document_Structure_Graph3/relationships.csv')\n",
    "rel.to_excel('../data/neo4j_data/Document_Structure_Graph3/relationships.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa155d6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
